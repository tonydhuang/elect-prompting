{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2afad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = \"{:.3f}\".format\n",
    "\n",
    "csvPath = './data'\n",
    "resultPath = './results'\n",
    "url = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f59f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_post_request(url: str, prompt: str):\n",
    "    headers = {\"Content-Type\": \"application/json; charset=utf-8\"}\n",
    "    payload = { \n",
    "     \"model\": \"llama3\", \n",
    "     \"prompt\": prompt,\n",
    "     \"stream\": False,\n",
    "     \"options\": {\n",
    "         \"temperature\": 1.0\n",
    "     }\n",
    "    }\n",
    "    res = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    return res\n",
    "def ollama_generate(prompt='', n=1):\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        resp = ollama_post_request(url=url, prompt=prompt)\n",
    "        result = resp.json()\n",
    "        scores.append(result['response']) \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageScores(arr):\n",
    "    arrInt = []\n",
    "    error = 0\n",
    "    for a in arr:\n",
    "        if type(a) is str and a.isnumeric() and int(a) < 4 and int(a) > -1:\n",
    "            arrInt.append(int(a))\n",
    "        else:\n",
    "            error += 1\n",
    "    if error > 2:\n",
    "        print('error counter: ', error)\n",
    "    return np.round(sum(arrInt) / len(arrInt), 3) if len(arrInt) > 0 else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57172205",
   "metadata": {},
   "source": [
    "## Demonstration examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98298e34",
   "metadata": {},
   "source": [
    "### Simple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b838e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deomonstraion examples for QA evaluation\n",
    "simpleSample1 = \"\"\"\n",
    "    **Product Question**: Is this difficult to assemble? If yes, what kind of tools are required for assembly?\n",
    "    **Model Generated Answer**: We typically ship within 1-2 business days.\n",
    "    **Rating Score**: 0\n",
    "\"\"\"\n",
    "simpleSample2 = \"\"\"\n",
    "    **Product Question**: How difficult is it to put the roll of paper towels on and off the roller? Mine will be placed on the side of my clothes dryer, in a cramped space.\n",
    "    **Model Generated Answer**: it's not that difficult in a small kitchen.\n",
    "    **Rating Score**: 1     \n",
    "\"\"\"\n",
    "simpleSample3 = \"\"\"\n",
    "    **Product Question**: How wide is this seat?\n",
    "    **Model Generated Answer**: it is less than 10\" wide.\n",
    "    **Rating Score**: 2\n",
    "\"\"\"\n",
    "simpleSample4 = \"\"\"\n",
    "    **Product Question**: Is this difficult to clean?\n",
    "    **Model Generated Answer**: it can be a little tedious to clean.\n",
    "    **Rating Score**: 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21af218",
   "metadata": {},
   "source": [
    "### ELECT samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6801b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those demonstration examples were originally generated by chatGPT. \n",
    "# We refine it with a more detailed thinking process specific to each certain product question.\n",
    "# See our paper with more detials. \n",
    "electSample1 = \"\"\"\n",
    "    **Product Question**: Is this difficult to assemble? If yes, what kind of tools are required for assembly?\n",
    "    **Model Generated Answer**: We typically ship within 1-2 business days.\n",
    "    **Chain of Thought**:\n",
    "    ***1. Understanding the Product Question***\n",
    "    The question asks whether the product is difficult to assemble and, if so, what kind of tools are required for assembly. The user is looking for specific information regarding the assembly process.\n",
    "    ***2. Analyzing the Model Generated Answer***\n",
    "    The model generated answer states, \"We typically ship within 1-2 business days.\"\n",
    "    The model generated answer is completely irrelevant to the question about assembly difficulty and tools needed. It addresses shipping information instead of the assembly process.\n",
    "    ***3. Rating the Model Generated Answer Based on the Evaluation Criteria***\n",
    "    Score 0: The model generated answer does not relate to the question at all. It fails to address assembly difficulty or the tools required for assembly.\n",
    "    ***4. Conclusion***\n",
    "    The model generated answer does not answer the user's question and provides unrelated information about shipping.\n",
    "    Rating: 0 (Irrelevant to the question).\n",
    "\"\"\"\n",
    "electSample2 = \"\"\"\n",
    "    **Product Question**: How difficult is it to put the roll of paper towels on and off the roller? Mine will be placed on the side of my clothes dryer, in a cramped space.\n",
    "    **Model Generated Answer**: it's not that difficult in a small kitchen.\n",
    "    **Chain of Thought**:\n",
    "    ***1. Understanding the Product Question***\n",
    "    The question asks about the difficulty of placing a roll of paper towels on and off the roller, particularly in a cramped space next to a clothes dryer. The user is looking for specific insights into the ease of use of the product in a limited area.\n",
    "    ***2. Analyzing the Model Generated Answer***\n",
    "    The model generated answer states, \"it's not that difficult in a small kitchen.\"\n",
    "    While the model generated answer suggests that the task is relatively easy, it does not specifically address the user's context of a cramped space next to a clothes dryer.\n",
    "    The mention of a \"small kitchen\" does not directly correlate to the user's question about difficulty in a cramped space, which could potentially be different.\n",
    "    ***3. Rating the Model Generated Answer Based on the Evaluation Criteria***\n",
    "    Score 1: The model generated answer is related to the question but does not directly answer it. It suggests ease of use but fails to consider the specific context of the user (a cramped space next to a dryer) and does not provide a definitive answer regarding the difficulty in that situation.\n",
    "    ***4. Conclusion***\n",
    "    The model generated answer is somewhat relevant but lacks specificity and does not adequately address the user’s unique context or concern.\n",
    "    Rating: 1 (Related but didn’t answer the question).    \n",
    "\"\"\"\n",
    "electSample3 = \"\"\"\n",
    "    **Product Question**: How wide is this seat?\n",
    "    **Model Generated Answer**: it is less than 10\" wide.\n",
    "    **Chain of Thought**:\n",
    "    ***1. Understanding the Product Question***\n",
    "    The question asks specifically about the width of the seat. The user is looking for a clear measurement indicating how wide the seat is.\n",
    "    ***2. Analyzing the Model Generated Answer***\n",
    "    The model generated answer states, \"it is less than 10\" wide.\"\n",
    "    The model generated answer provides a direct measurement related to the user's question, indicating that the seat width is under 10 inches. However, it does not give a specific measurement, which might leave the user wanting more precise information.    \n",
    "    ***3. Rating the Model Generated Answer Based on the Evaluation Criteria***\n",
    "    Score 2: The response somewhat answers the question by giving an indication of the width. While it does not provide a specific width, the phrase \"less than 10 inches\" gives a general idea of the size, which is still helpful.\n",
    "    ***4. Conclusion***\n",
    "    The model generated answer is relevant and provides useful information about the seat width, even if it lacks a specific measurement.\n",
    "    Rating: 2 (Somehow answered the question).\n",
    "\"\"\"\n",
    "electSample4 = \"\"\"\n",
    "    **Product Question**: Is this difficult to clean?\n",
    "    **Model Generated Answer**: it can be a little tedious to clean.\n",
    "    **Chain of Thought**:\n",
    "    ***1. Understanding the Product Question***\n",
    "    The question asks, \"Is this difficult to clean?\" The user is seeking information about the ease or difficulty of cleaning the product. The expected answer should clearly address whether the product is easy or difficult to clean.\n",
    "    ***2. Analyzing the Model Generated Answer***\n",
    "    The model generated answer says, \"it can be a little tedious to clean.\"\n",
    "    The model generated answer indicates that cleaning the product may not be easy and can take some effort, which aligns with the user's inquiry about difficulty.\n",
    "    The phrase \"a little tedious\" implies some difficulty, though not extreme, and provides a direct answer to the question.\n",
    "    ***3. Rating the Model Generated Answer Based on the Evaluation Criteria***\n",
    "    Score 3: The model generated answer directly answers the question by explaining that cleaning the product is somewhat difficult or tedious. It addresses the key aspect of the user's question.\n",
    "    ***4. Conclusion***\n",
    "    The model generated answer is relevant, clear, and directly addresses the question about difficulty in cleaning.\n",
    "    Rating: 3 (Directly answers the question).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69578266",
   "metadata": {},
   "source": [
    "## CoT and prompting templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2dbe2b",
   "metadata": {},
   "source": [
    "### Generating CoT steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4478a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CoT steps were created by ChatGPT in 2024 \n",
    "# The the following prompt was used to create the general CoT steps\n",
    "cot_gen_v1 = \"\"\"\n",
    "    You are a product question answering task evaluator.\n",
    "    You will be given a product question and a model generated answer.\n",
    "    Your task is to rate how well the model generated answer can answer the question based on the evaluation criteria.\n",
    "\n",
    "    Evaluation Criteria: Rating Score (0-3)\n",
    "    0: the answer was irrelevant to the question;\n",
    "    1: the answer was related but didn't answer the question;\n",
    "    2: the answer somehow answered the question;\n",
    "    3: the answer directly answer the question.\n",
    "    \n",
    "    Please generate the Chain of Thought steps for the evaluation task\n",
    "\"\"\"\n",
    "\n",
    "# General CoT steps\n",
    "general_cot = \"\"\"\n",
    "    1. Understanding the Product Question\n",
    "    Clearly define the product-related question being asked.\n",
    "    Identify any specific details or requirements the question is asking for (e.g., features, specifications, price, availability).\n",
    "\n",
    "    2. Analyzing the Model Generated Answer\n",
    "    Check whether the model generated answer addresses the key aspects of the question.\n",
    "    Determine if the model generated answer provides the necessary information (or if something is missing).\n",
    "    Verify if the model generated answer is directly related to the product in question.\n",
    "    Evaluate if the model generated answer includes irrelevant or extraneous information.\n",
    "\n",
    "    3. Rating the Model Generated Answer Based on the Evaluation Criteria\n",
    "    Score 0: If the model’s response is entirely irrelevant or doesn’t address any part of the question.\n",
    "    Score 1: If the model’s response is somewhat related to the question, but it doesn’t answer it.\n",
    "    Score 2: If the model’s response partially answers the question but is incomplete or lacks clarity.\n",
    "    Score 3: If the model’s response fully and directly answers the question with clear and relevant information.\n",
    "\n",
    "    4. Conclusion\n",
    "    Based on the analysis, assign a score (0–3) to the model generated answer.\n",
    "\"\"\"\n",
    "\n",
    "# The the following prompt could be used to create the CoT steps against a specific question.\n",
    "# Parameter 1: cot_gen\n",
    "# parameter 2: a question (in the samples above)\n",
    "# parameter 3: generated answer (in the samples above)\n",
    "cot_gen_v2 = \"\"\"\n",
    "    You are a product question answering task evaluator.\n",
    "    You will be given a product question and a model generated answer.\n",
    "    Your task is to rate how well the model generated answer can answer the question based on the evaluation criteria.\n",
    "\n",
    "    Evaluation Criteria: Rating Score (0-3)\n",
    "    0: the answer was irrelevant to the question;\n",
    "    1: the answer was related but didn't answer the question;\n",
    "    2: the answer somehow answered the question;\n",
    "    3: the answer directly answer the question.\n",
    "    \n",
    "    Please break down the evaluation process using the Chain of Thought steps for the given question and generated answer:\n",
    "    \n",
    "    Chain of Thought:\n",
    "    {}\n",
    "    \n",
    "    Question:\n",
    "    {}\n",
    "    \n",
    "    Generated answer:\n",
    "    {}\n",
    "\"\"\"\n",
    "\n",
    "# CoT steps for specific QA pair\n",
    "cot_sample = \"\"\"\n",
    "    1. Understanding the Product Question\n",
    "    The question asks, \"Is this difficult to clean?\" The user is seeking information about the ease or difficulty of cleaning the product. The expected answer should clearly address whether the product is easy or difficult to clean.\n",
    "    \n",
    "    2. Analyzing the Model Generated Answer\n",
    "    The model-generated answer says, \"it can be a little tedious to clean.\"\n",
    "    The phrase \"a little tedious\" suggests that cleaning is not extremely difficult but requires some effort.\n",
    "\n",
    "    3. Rating the Model Generated Answer Based on the Evaluation Criteria\n",
    "    Score 3: The model generated answer directly answers the question by explaining that cleaning the product is somewhat difficult or tedious. It addresses the key aspect of the user's question.\n",
    "    \n",
    "    4. Conclusion\n",
    "    The model generated answer is relevant, clear, and directly addresses the question about difficulty in cleaning.\n",
    "    Rating: 3 (Directly answers the question).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d4e4a",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe39a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_output = \"Please do not generate any opening, closing, and explanations. The response should be one word with a number between 0 and 3 only.\"\n",
    "\n",
    "# input: general CoT steps\n",
    "cotTemplete = \"\"\"\n",
    "    Chain of Thought:\n",
    "    {}\n",
    "\"\"\"\n",
    "\n",
    "# Parameter 1. desired output format\n",
    "# Parameter 2. cot\n",
    "# Parameter 3. demonstrain examples\n",
    "# Parameter 4. input: product question\n",
    "# Parameter 5. input: model generated answer\n",
    "\n",
    "generic_prompt = \"\"\"\n",
    "    You are a product question answering task evaluator.\n",
    "    You will be given a product question, and a model generated answer.\n",
    "    Your task is to rate how well the model generated answer can answer the question based on the evaluation criteria.\n",
    "    {}\n",
    "\n",
    "    Evaluation Criteria: Rating Score (0-3)\n",
    "    0: the answer was irrelevant to the question;\n",
    "    1: the answer was related but didn't answer the question;\n",
    "    2: the answer somehow answered or partially answered the question;\n",
    "    3: the answer directly answer the question.\n",
    "    \n",
    "    {}\n",
    "    \n",
    "    {}\n",
    "    \n",
    "    Product Question:\n",
    "    {}\n",
    "\n",
    "    Model Generated Answer:\n",
    "    {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e52cc04",
   "metadata": {},
   "source": [
    "## Experiments -- Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'garden'\n",
    "csvFile = 'QA_{}'.format(product)\n",
    "llamaDF = pd.read_csv('{}/{}.csv'.format(csvPath, csvFile), sep='\\t')\n",
    "print('result original:', len(llamaDF))\n",
    "\n",
    "questions = llamaDF['question'].tolist()\n",
    "answers = llamaDF['answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "all_scores = []\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "agents = 10 # 20\n",
    "i = 0\n",
    "    \n",
    "cot = cotTemplete.format(general_cot)\n",
    "\n",
    "# examplesTemplete = \"\"\"\n",
    "#     Here are some examples using chain of thought for the evaluation process:\n",
    "    \n",
    "#     ###Example One###:\n",
    "#     {}\n",
    "#     ###Example Two###:\n",
    "#     {}\n",
    "#     ###Example Three###:\n",
    "#     {}\n",
    "#     ###Example Four###:\n",
    "#     {}\n",
    "# \"\"\"\n",
    "# electExamples = examplesTemplete.format(electSample1, electSample2, electSample3, electSample4)\n",
    "\n",
    "examplesTemplete = \"\"\"\n",
    "    Here are some examples using chain of thought for the evaluation process:\n",
    "    \n",
    "    ###Example One###:\n",
    "    {}\n",
    "\"\"\"\n",
    "electExamples = examplesTemplete.format(electSample4)\n",
    "\n",
    "# simpleExamplesTemplete = \"\"\"\n",
    "#     Here are some examples:\n",
    "    \n",
    "#     ###Example One###:\n",
    "#     {}\n",
    "#     ###Example Two###:\n",
    "#     {}\n",
    "#     ###Example Three###:\n",
    "#     {}\n",
    "#     ###Example Four###:\n",
    "#     {}\n",
    "# \"\"\"\n",
    "# simpleExamples = simpleExamplesTemplete.format(simpleSample1, simpleSample2, simpleSample3, simpleSample4)\n",
    "\n",
    "simpleExamplesTemplete = \"\"\"\n",
    "    Here are some examples:\n",
    "    \n",
    "    ###Example One###:\n",
    "    {}\n",
    "\"\"\"\n",
    "simpleExamples = simpleExamplesTemplete.format(simpleSample4)\n",
    "\n",
    "### Table 2\n",
    "# ELECT: prompt = generic_prompt.format(llama_output, electExamples, \"\", q, a) ## using electSample4\n",
    "# G-Eval: prompt = generic_prompt.format(llama_output, cot, \"\", q, a)\n",
    "# DP(direct prompt): prompt = generic_prompt.format(llama_output, \"\" , \"\", q, a)\n",
    "# One-shot DP: prompt = generic_prompt.format(llama_output, simpleExamples, \"\", q, a) ## using simpleSample4\n",
    "\n",
    "for q, a in tqdm(zip(questions, answers)):\n",
    "    prompt = generic_prompt.format(llama_output, simpleExamples, \"\", q, a)\n",
    "    if i == 0:\n",
    "        print(prompt)\n",
    "        i += 1\n",
    "    scores = ollama_generate(prompt=prompt, n=agents)\n",
    "    all_scores.append(averageScores(scores))\n",
    "    results.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedStr = 'llama_dp4'\n",
    "llamaDF[fixedStr] = results\n",
    "llamaDF['{}_score'.format(fixedStr)] = all_scores\n",
    "llamaDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5946a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=======> ', product)\n",
    "# human judgement\n",
    "human_judgements = list(llamaDF['annotated'])\n",
    "human_judgements = [float(float_formatter(num)) for num in np.round(human_judgements, 2)]\n",
    "for metric in [\n",
    "    'llama_elect4_score',\n",
    "    'llama_geval_score',\n",
    "    'llama_dp_score',\n",
    "    'llama_dp4_score'\n",
    "    ]:\n",
    "    metric_score = list(llamaDF['{}'.format(metric)])\n",
    "\n",
    "    pScore, pvpScore = pearsonr(metric_score, human_judgements)\n",
    "    sScore, pvsScore = spearmanr(metric_score, human_judgements)\n",
    "    kScore, pvkScore = kendalltau(metric_score, human_judgements)\n",
    "    print('%s: spearman %.3f pv %.3f, pearson %.3f pv %.3f, kendalltau %.3f pv %.3f' % \n",
    "          (metric, sScore, pvsScore, pScore, pvpScore, kScore, pvkScore)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = 'llama3_{}_{}_{}.csv'.format(product, 'Table2', agents)\n",
    "llamaDF.to_csv('{}/{}'.format(resultPath, csvFile), index=None, sep='\\t')\n",
    "\n",
    "print(csvFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
